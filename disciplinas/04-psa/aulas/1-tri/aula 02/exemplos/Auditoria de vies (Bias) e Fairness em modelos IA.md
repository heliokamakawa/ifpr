# Projeto PrÃ¡tico: Auditoria de ViÃ©s (Bias) e Fairness em Modelos de IA  
Tema: Qualidade de Software para Sistemas de IA (Responsible AI / SE4AI)

Este projeto Ã© um guia passo a passo para equipes realizarem um experimento replicÃ¡vel de **detecÃ§Ã£o e mitigaÃ§Ã£o de viÃ©s algorÃ­tmico** em um modelo de Machine Learning.

ReferÃªncia conceitual complementar usada neste projeto:  
ğŸ”— **AI Bias Audit: 7 Steps to Detect Algorithmic Bias** â€” Optiblack Insights  
https://optiblack.com/insights/ai-bias-audit-7-steps-to-detect-algorithmic-bias

Este texto serve como **checklist conceitual** e sequÃªncia recomendada de tarefas para orientar a auditoria de modelos de IA.

---

## 1) O que vocÃªs vÃ£o â€œtestarâ€ exatamente

VocÃªs nÃ£o estÃ£o testando â€œunit testsâ€.  
VocÃªs estÃ£o testando um atributo de qualidade nÃ£o funcional do sistema de IA:

- O modelo decide de forma **desigual entre grupos**?
- Essa desigualdade pode ser medida?
- Pode ser reduzida? Qual o custo em performance?

Os 7 passos para auditar viÃ©s, conforme explicados no artigo de apoio, sÃ£o exatamente o que orienta o procedimento:  
1) Entender os dados,  
2) Avaliar o modelo,  
3) Medir fairness,  
4) Testar vieses combinados,  
5) Verificar impacto no mundo real,  
6) Documentar e relatar,  
7) Propor melhorias.  
(*ver link: Optiblack AI Bias Audit*)

---

## 2) PrÃ©-requisitos mÃ­nimos do experimento

Para treinar e auditar um modelo, vocÃªs precisam de:

1. Um dataset tabular (CSV) com:
   - Colunas de atributos (features)
   - Uma coluna alvo (label) binÃ¡ria (0/1)
   - Pelo menos uma coluna sensÃ­vel (para auditoria de fairness)
2. Um modelo de classificaÃ§Ã£o simples (baseline)
3. MÃ©tricas de desempenho (ex.: F1)
4. MÃ©tricas de fairness (ex.: Disparate Impact, Equal Opportunity)
5. Uma tÃ©cnica de mitigaÃ§Ã£o (prÃ©, in ou pÃ³s-processamento)
6. Um relatÃ³rio de auditoria (protocolo + resultados + anÃ¡lise crÃ­tica)

Este guia segue a sequÃªncia lÃ³gica recomendada de etapas descritas no texto de apoio, ajustadas para a disciplina.

---

## 3) Escolha do dataset: use dados prontos

Escolham um dataset clÃ¡ssico e pÃºblico:

### Caminho A â€” Adult Income
- PrevÃª renda
- Atributo sensÃ­vel: sexo (e possivelmente raÃ§a)

### Caminho B â€” German Credit
- PrevÃª risco de crÃ©dito
- Atributo sensÃ­vel: idade ou sexo

### Caminho C â€” COMPAS
- PrevÃª risco de reincidÃªncia
- Atributo sensÃ­vel: raÃ§a

### Caminho D â€” EvasÃ£o escolar
- PrevÃª evasÃ£o
- Atributo sensÃ­vel: renda, gÃªnero ou tipo de escola

A escolha deve incluir um atributo sensÃ­vel para que a auditoria de fairness esteja bem definida â€” conforme passo 1 do processo de auditoria indicado no texto de referÃªncia.

---

## 4) DefiniÃ§Ã£o do recorte experimental

Escolham um recorte claro baseado neste checklist do artigo de apoio:

### Recorte 1 â€” Auditoria simples (rÃ¡pido)
- Treinar modelo
- Medir fairness (2 mÃ©tricas)
- Aplicar mitigaÃ§Ã£o
- Medir novamente

### Recorte 2 â€” ComparaÃ§Ã£o de modelos
- Testar 2â€“3 modelos
- Medir fairness e performance
- Comparar trade-off

### Recorte 3 â€” Comparar tÃ©cnicas de mitigaÃ§Ã£o
- Testar 2 mitigadores
- Analisar impacto na justiÃ§a e performance

### Recorte 4 â€” Interseccionalidade
- Avaliar fairness sob mais de um atributo sensÃ­vel

Estes sÃ£o alinhados com os 7 passos de auditoria: medir, comparar e propor melhorias.

---

## 5) PreparaÃ§Ã£o dos dados

Antes de treinar, vocÃªs devem:

1. Garantir que o label Ã© binÃ¡rio
2. Separar atributos (X) e alvo (y)
3. Tratar valores ausentes
4. Codificar categorias
5. Separar treino/teste
6. Definir grupo sensÃ­vel

Esse procedimento corresponde aos primeiros passos de auditoria â€” conhecer e preparar os dados adequadamente para avaliaÃ§Ã£o de fairness.

---

## 6) Treino do baseline (modelo simples)

Treinem um modelo de classificaÃ§Ã£o simples (por exemplo, Logistic Regression ou Random Forest).  
ApÃ³s o treino, avaliem performance no conjunto de teste:

- F1-score  
- Precision  
- Recall  

Isso corresponde ao passo â€œexaminar o modelo de IAâ€ no artigo de apoio.

---

## 7) Auditoria de fairness

Aqui vocÃªs vÃ£o medir se hÃ¡ vieses no modelo.  
Recomendam-se pelo menos duas mÃ©tricas:

- Disparate Impact  
- Statistical Parity Difference  
- Equal Opportunity Difference  

A ideia segue o passo â€œmedir fairnessâ€ e â€œtestar vieses combinadosâ€ do texto de apoio.

Registrem os resultados em tabela e expliquem o significado de cada mÃ©trica no contexto do problema.

---

## 8) MitigaÃ§Ã£o do viÃ©s

ApÃ³s medir a desigualdade, testem uma tÃ©cnica de mitigaÃ§Ã£o:

- PrÃ©-processamento
- In-processamento
- PÃ³s-processamento

Reavaliem as mÃ©tricas de fairness e performance.

Isto corresponde ao passo de â€œpropor melhoriasâ€ indicado no texto de apoio e mostra a aplicaÃ§Ã£o prÃ¡tica de engenharia de qualidade de software em IA.

---

## 9) ComparaÃ§Ã£o e anÃ¡lise

Elaborem uma tabela comparando antes/depois:

- Performance (F1, precision, recall)
- MÃ©tricas de fairness

DiscussÃ£o esperada:

- O viÃ©s diminuiu?
- Qual o custo em performance?
- Qual tÃ©cnica de mitigaÃ§Ã£o Ã© mais adequada?

Esta etapa encerra a auditoria e segue os passos finais do checklist de auditoria: medir, comparar e relatar.

---

## 10) Protocolo experimental (obrigatÃ³rio)

O documento protocol.md deve incluir:

1. Problema e objetivo
2. Dataset e justificativa
3. Atributo sensÃ­vel e definiÃ§Ã£o de grupos
4. Modelo(s) e justificativa
5. MÃ©tricas de performance
6. MÃ©tricas de fairness
7. TÃ©cnica de mitigaÃ§Ã£o escolhida
8. ExecuÃ§Ã£o (treino/teste)
9. Resultados e tabela comparativa
10. DiscussÃ£o crÃ­tica e vulnerabilidades

A ordem corresponde diretamente aos passos de auditoria descritos no texto de apoio.

---

## 11) Entrega mÃ­nima do projeto

- Dataset documentado
- Baseline treinado
- MÃ©tricas de performance
- MÃ©tricas de fairness
- MitigaÃ§Ã£o testada
- ComparaÃ§Ã£o antes/depois
- Protocolo completo
- RepositÃ³rio organizado

---

## 12) ObservaÃ§Ã£o sobre o texto de apoio

O artigo â€œAI Bias Audit: 7 Steps to Detect Algorithmic Biasâ€ nÃ£o Ã© cientÃ­fico peer-review, mas Ã© **atual e Ãºtil metodologicamente**, pois apresenta um checklist de tarefas que orientam o desenvolvimento do projeto. Ele funciona como uma sequÃªncia de etapas prÃ¡ticas que podem ser conectadas com as mÃ©tricas e decisÃµes de engenharia de qualidade de software em IA.

ğŸ”— Link de referÃªncia:  
https://optiblack.com/insights/ai-bias-audit-7-steps-to-detect-algorithmic-bias

---

## Resultado esperado

Ao final, espera-se que cada grupo seja capaz de:

- Formular um protocolo replicÃ¡vel
- Auditar um modelo de IA de forma objetiva
- Interpretar mÃ©tricas de fairness
- Comparar trade-offs entre justiÃ§a e desempenho
- Relatar conclusÃµes sÃ³lidas com base em dados

Esse Ã© um exercÃ­cio de **engenharia de qualidade aplicada a sistemas de IA** â€” moderno, replicÃ¡vel e relevante.

